{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')['data']\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')['data']\n",
    "NUM_TOPICS = 30\n",
    "NUM_NEIGHBORS = 10\n",
    "BUCKET = 'sagemaker-sbn'\n",
    "PREFIX = '20newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newsgroups_train)):\n",
    "    newsgroups_train[i] = strip_newsgroup_header(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_quoting(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_footer(newsgroups_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "newidx = np.random.permutation(idx) # this will be the labels fed into the KNN model for training\n",
    "# Need to store these permutations:\n",
    "\n",
    "vectors = vectors[newidx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into training and validation data\n",
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "val_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "\n",
    "print(train_vectors.shape,val_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=2, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key') \n",
    "ntm.fit({'train': s3_train, 'test': s3_val_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for item in np.array(vectors.todense()):\n",
    "    np.shape(item)\n",
    "    results = ntm_predictor.predict(item)\n",
    "    predictions.append(np.array([prediction['topic_weights'] for prediction in results['predictions']]))\n",
    "    \n",
    "predictions = np.array([np.ndarray.flatten(x) for x in predictions])\n",
    "topicvec = train_labels[newidx]\n",
    "topicnames = [categories[x] for x in topicvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"np.savetxt('trainvectors.csv',\n",
    "           vectors.todense(),\n",
    "           delimiter=',',\n",
    "           fmt='%i')\n",
    "batch_prefix = '20newsgroups/batch'\n",
    "\n",
    "train_s3 = sess.upload_data('trainvectors.csv', \n",
    "                            bucket=bucket, \n",
    "                            key_prefix='{}/train'.format(batch_prefix))\n",
    "print(train_s3)\n",
    "batch_output_path = 's3://{}/{}/test'.format(bucket, batch_prefix)\n",
    "\n",
    "ntm_transformer = ntm.transformer(instance_count=1,\n",
    "                                  instance_type ='ml.m4.xlarge',\n",
    "                                  output_path=batch_output_path\n",
    "                                 )\n",
    "ntm_transformer.transform(train_s3, content_type='text/csv', split_type='Line')\n",
    "ntm_transformer.wait()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive $ntm_transformer.output_path ./\n",
    "#!head -c 5000 trainvectors.csv.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=5000)\n",
    "tsne_results = tsne.fit_transform(predictions)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "tsnedf = pd.DataFrame()\n",
    "tsnedf['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsnedf['tsne-2d-two'] = tsne_results[:,1]\n",
    "tsnedf['Topic']=topicnames\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue='Topic',\n",
    "    palette=sns.color_palette(\"hls\", NUM_TOPICS),\n",
    "    data=tsnedf,\n",
    "    legend=\"full\",\n",
    "    fit_reg=False\n",
    ")\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = newidx \n",
    "labeldict = dict(zip(newidx,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "print('train_features shape = ', predictions.shape)\n",
    "print('train_labels shape = ', labels.shape)\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, predictions, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "key = 'knn/train'\n",
    "fname = os.path.join(prefix, key)\n",
    "print(fname)\n",
    "boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.c4.xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': predictions.shape[1],\n",
    "    'k': NUM_NEIGHBORS,\n",
    "    'sample_size': predictions.shape[0],\n",
    "    'predictor_type': 'classifier' ,\n",
    "    'index_metric':'COSINE'\n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/knn/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name,\n",
    "                                        accept=\"application/jsonlines; verbose=true\")\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor\n",
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "print('setting up the endpoint..')\n",
    "knn_predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text):\n",
    "    text = strip_newsgroup_header(text)\n",
    "    text = strip_newsgroup_quoting(text)\n",
    "    text = strip_newsgroup_footer(text)\n",
    "    return text    \n",
    "    \n",
    "test_data_prep = []\n",
    "for i in range(len(newsgroups_test)):\n",
    "    test_data_prep.append(preprocess_input(newsgroups_test[i]))\n",
    "test_vectors = vectorizer.fit_transform(test_data_prep)\n",
    "\n",
    "test_vectors = np.array(test_vectors.todense())\n",
    "test_topics = []\n",
    "for vec in test_vectors:\n",
    "    test_result = ntm_predictor.predict(vec)\n",
    "    test_topics.append(test_result['predictions'][0]['topic_weights'])\n",
    "\n",
    "topic_predictions = []\n",
    "for topic in test_topics:\n",
    "    result = knn_predictor.predict(topic)\n",
    "    cur_predictions = np.array([int(result['labels'][i]) for i in range(len(result['labels']))])\n",
    "    topic_predictions.append(cur_predictions[::-1][:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own k.\n",
    "def plot_topic_distribution(topic_num, k = 5):\n",
    "    \n",
    "    closest_topics = [predictions[labeldict[x]] for x in topic_predictions[topic_num][:k]]\n",
    "    closest_topics.append(np.array(test_topics[topic_num]))\n",
    "    closest_topics = np.array(closest_topics)\n",
    "    df = pd.DataFrame(closest_topics.T)\n",
    "    df.rename(columns ={k:\"Test Document Distribution\"}, inplace=True)\n",
    "    fs = 12\n",
    "    df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "    plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "    plt.xlabel('Topic ID', fontsize=fs+2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
